{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2a747369b91572b4ada4e32643e31bca",
          "grade": false,
          "grade_id": "Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S78YoCEOor7H"
      },
      "source": [
        "#**EE769 Introduction to Machine Learning**\n",
        "\n",
        "#Assignment 1: Gradient Descent, Linear Regression, and Regularization\n",
        "\n",
        "\n",
        "**Template and Instructions**\n",
        "\n",
        "\n",
        "\n",
        "1. Up to two people can team up, but only one should submit, and both should understand the entire code.\n",
        "2. Every line of code should end in a comment explaining the line\n",
        "3. It is recommended to solve the assignment in Google Colab.\n",
        "Write your roll no.s separated by commas here: \n",
        "4. Write your names here: SRINITHI S, VAISHNAVI J\n",
        "5. There are two parts to the assignment. In the Part 1, the code format has to be strictly followed to enable auto-grading. In the second part, you can be creative.\n",
        "6. **You can discuss with other groups or refer to the internet without being penalized, but you cannot copy their code and modify it. Write every line of code and comment on your own.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cae69541658b406d495bcffe0b7c0932",
          "grade": false,
          "grade_id": "cell-aa1aa8c8181f8810",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "z8SPmxkbor7T"
      },
      "source": [
        "#**Part 1 begins ...**\n",
        "**Instructions to be strictly followed:**\n",
        "\n",
        "1. Do not add any code cells or markdown cells until the end of this part. Especially, do not change the blocks that say \"TEST CASES, DO NOT CHANGE\"\n",
        "2. In all other cells only add code where it says \"CODE HERE\".\n",
        "3. If you encounter any raise NotImplementedError() calls you may comment them out.\n",
        "\n",
        "We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ecbf8e96219ab11b7716eaae61cb7f9",
          "grade": false,
          "grade_id": "cell-1e940101f37c7af2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bIMay61Gor7V"
      },
      "source": [
        "## Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe70877a859ee0a10e9d591778022f8a",
          "grade": false,
          "grade_id": "Import_Statements",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SWPy55r1or7X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e36a214573fc40f26b45e72215d61375",
          "grade": false,
          "grade_id": "Normalize_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wq8fsa6Nor7b"
      },
      "source": [
        "## Normalize function \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42293dda356d746ac19e9b6d81106261",
          "grade": false,
          "grade_id": "Normalize_Function",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1iKQnxsFor7d"
      },
      "outputs": [],
      "source": [
        "def Normalize(X): # Output should be a normalized data matrix of the same dimension\n",
        "    '''\n",
        "    Normalize all columns of X using mean and standard deviation\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    m = np.mean(X,axis=0)   # Mean of each axis of the input X. axis=0 in means first axis. \n",
        "    s = np.std(X,axis=0)    # Standard deviation of each axis of X.\n",
        "    norm = (X-m)/s          # Normalization of X. Each element of the of X is operated with corresponding m and s of same axis. \n",
        "    return norm             # Returning normalized X as output.\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7797dd606a68f028c945acd08850d108",
          "grade": true,
          "grade_id": "Normalize_Test",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ybFg1HURor7g"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - 1 dimensional array'''\n",
        "#X=np.array([[1,2,3],[3,4,5],[7,8,9]])\n",
        "X1=np.array([1,2,3])\n",
        "np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)\n",
        "''' case 2 - 2 dimensional array'''\n",
        "X2=np.array([[4,7,6],[3,8,9],[5,11,10]])\n",
        "np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))\n",
        "''' case 3 - 1 dimensional array with float'''\n",
        "X3=np.array([5.5,6.7,3.2,6.7])\n",
        "np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "63ee774a26a0438a0b2f5ec298eac80e",
          "grade": false,
          "grade_id": "cell-66ab98f84d3c58fa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "C5oTeb4jor7j"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "Given X and w, compute the predicted output. Do not forget to add 1's in X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5976e32bd96bc4bdc2b2efd61e98441c",
          "grade": false,
          "grade_id": "Prediction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ujaTPZoVor7l"
      },
      "outputs": [],
      "source": [
        "def Prediction (X, w): # Output should be a prediction vector y\n",
        "    '''\n",
        "    Compute Prediction given an input datamatrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s \n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    unit = np.ones((len(X),1),dtype='int')           # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                    # Horizontally concatenates X with X1 \n",
        "    y = np.dot(X_final,w)                            # Matrix multiplication of X_final and w to get the estimate y\n",
        "    return y                                         # Returning predicted y as output for given X and w.\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "741f088432fb2ac3c49d3c9711d5c9c5",
          "grade": true,
          "grade_id": "PredictionTest",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "A5R_gEGQor7o"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - Known input output matrix and weights 1'''\n",
        "X1 = np.array([[3,2],[1,1]])\n",
        "w1 = np.array([2,1,1]) \n",
        "np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "227f6e0da2f8c6d631599c44041e3b65",
          "grade": false,
          "grade_id": "cell-6fda2832d5967072",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2ngus4Hror7p"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "Code the four  loss functions:\n",
        "\n",
        "1. MSE loss is only for the error\n",
        "2. MAE loss is only for the error\n",
        "3. L2 loss is for MSE and L2 regularization, and can call MSE loss\n",
        "4. L1 loss is for MSE and L1 regularization, and can call MSE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c3e17b8559f0dfdfecdc8d242aba152",
          "grade": false,
          "grade_id": "MSE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7mTqWvFTor7r"
      },
      "outputs": [],
      "source": [
        "def MSE_Loss (X, t, w, lamda =0): # Ouput should be a single number\n",
        "    '''\n",
        "    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake. \n",
        "    This allows us to call all loss functions with the same input format.\n",
        "    \n",
        "    You are encouraged read about default arguments by yourself online if you're not familiar.\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "\n",
        "    # Ones must be added to input X to account for the bias terms\n",
        "    unit = np.ones((len(X),1),dtype='int')             # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                      # Horizontally concatenates X with unit \n",
        "    y = np.array(np.dot(X_final,w))                    # Matrix multiplication of X_final and w to get the estimate y\n",
        "    # Calculation of Mean Squared Error loss\n",
        "    MSE_loss = (1/len(y))*(np.sum(((y-t)**2)))         # Sum of squares of diff between estimate and target upon no. of datasets\n",
        "    return MSE_loss                                    # returning loss as output\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "923a1372a401b41d6ef6b0749a49ff66",
          "grade": true,
          "grade_id": "MSE_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "jlmolTDdor7s"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b0b3b381c7a3ac5d7dfbb5df647c0d85",
          "grade": false,
          "grade_id": "MAE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cB5L0SeVor7u"
      },
      "outputs": [],
      "source": [
        "def MAE_Loss (X, t, w, lamda = 0): # Output should be a single number\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "\n",
        "    # Ones must be added to input X to account for the bias terms\n",
        "    unit = np.ones((len(X),1),dtype='int')              # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                       # Horizontally concatenates X with unit \n",
        "    y=np.array(np.dot(X_final,w))                       # Matrix multiplication of X_final and w to get the estimate y\n",
        "    # Calculation of Mean Absolute Error loss\n",
        "    MAE_loss =(1/(len(y)))*(np.sum(abs(y-t)))            # Sum of absolute diff between estimate and target upon no. of datasets\n",
        "    return MAE_loss                                      # returning loss as output\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f11d5fde7220893a9809f2954d18c5e5",
          "grade": true,
          "grade_id": "MAE_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "RucPiNZMor7v"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8309762ea56bb6b24bbbf4a19cb16bc",
          "grade": false,
          "grade_id": "L2_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "m8PizVu5or7w"
      },
      "outputs": [],
      "source": [
        "def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)\n",
        "    ''' Need to specify what inputs are'''\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    # L2 norm of w is the total loss obtained by adding the L2 regularised term with MSE loss\n",
        "    MSE_loss=(MSE_Loss (X, t, w, lamda =0))       # Calling of MSE_Loss function to find the MSE loss\n",
        "    r = np.sum(w[0:-1]**2)                        # L2 regularisation- sum of squares of w expect the bias term\n",
        "    L2 = np.sqrt(r)*(lamda)                       # Square root of the L2 regularisation term and given lamda value multiplied \n",
        "    L2_loss = L2 + MSE_loss                       # Total loss = MSE loss + L2 regularisation term\n",
        "    return L2_loss                                # Total loss is returned as output\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a5a409056f6797cdf8cbfab61b138c37",
          "grade": true,
          "grade_id": "L2_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DrchNMBJor7y"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1af69c123815524cdc71c72e09ece638",
          "grade": false,
          "grade_id": "L1_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "phUmj2LWor70"
      },
      "outputs": [],
      "source": [
        "def L1_Loss (X, t, w, lamda): # Output should be a single number\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "\n",
        "    # L1 norm of w is the total loss obtained by adding the L1 regularised term with MSE loss\n",
        "    MSE_loss=(MSE_Loss (X, t, w, lamda =0))     # Calling of MSE_Loss function to find the MSE loss\n",
        "    r = np.sum(abs(w[0:-1]))                    # L1 regularisation- sum of absolute values of w expect the bias term\n",
        "    L1 = r * lamda                              # Product of Square root of the L1 regularisation term and given lamda value(extent of regularisation)\n",
        "    L1_loss = L1 + MSE_loss                     # Total loss = MSE loss + L1 regularisation term\n",
        "    return L1_loss                              # Total loss is returned as output\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40224dd69dd8e6499a67271c33701bf4",
          "grade": true,
          "grade_id": "L1_Loss_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "YUkRvxAAor70"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "855bd2ded492e567b20f1d9705d9a97a",
          "grade": false,
          "grade_id": "NRMSE_Metric",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kT1RIRLror71"
      },
      "outputs": [],
      "source": [
        "def NRMSE_Metric (X, t, w, lamda=0): # Output should be a single number. RMSE/std_dev(t)\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # NRMSE is the normalized root mean square error obtained by taking square root of MSE_loss and normalizing using standard deviation\n",
        "    MSE_loss = MSE_Loss (X, t, w, lamda =0)      # Calling of MSE_Loss function to find the MSE loss\n",
        "    RMSE = np.sqrt(MSE_loss)                     # Calculating root mean square error(RMSE) by taking square root of MSE loss\n",
        "    s = t.std()                                  # Calculating standard deviation to normalize RMSE\n",
        "    NRMSE = RMSE/s                               # Normalized root mean squared error (NRMSE) = RMSE divided by standard deviation\n",
        "    return NRMSE                                 # NRMSE is returned as output\n",
        "    \n",
        "    \n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "256795799ceb6dfa0b32ab75e8ef30c4",
          "grade": true,
          "grade_id": "NRMSE_Metric_Test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aPPSm0GHor72"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' Test case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc5df30e29bbe17d4bc7aa89210eb5cf",
          "grade": false,
          "grade_id": "Gradient_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "iJM3mXyDor73"
      },
      "source": [
        "## Gradient function\n",
        "Each Loss function will have its own gradient function:\n",
        "\n",
        "1. MSE gradient is only for the error\n",
        "2. MAE gradient is only for the error\n",
        "3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient\n",
        "4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0190492ee216edce701f93e697572fc1",
          "grade": false,
          "grade_id": "MSE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "R_ei5G4Kor73"
      },
      "outputs": [],
      "source": [
        "def MSE_Gradient (X, t, w, lamda=0): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    unit = np.ones((len(X),1),dtype='int')              # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                       # Horizontally concatenates X with unit \n",
        "    y=np.array(np.dot(X_final,w))                       # Matrix multiplication of X_final and w to get the estimate y\n",
        "    g=np.zeros(len(w))                                  # Initialisation of gradient as an array of zeros \n",
        "    for j in range(0,len(w)):                           # 'For' loop for considering one training set of X at a time\n",
        "        g[j]=(y-t).dot(X_final[:,j])                    # Gradient with respect Mean Squared error is calculated for each set 'j' of X\n",
        "    return g                                            # Gradient is returned\n",
        "\n",
        "    \n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d61466927592a8776a8bfa688472bad4",
          "grade": true,
          "grade_id": "MSE_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PdtCAg7xor74"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "49cc4e37cafad52f41853b4b0e71f89f",
          "grade": false,
          "grade_id": "MAE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "XA3MWH6ror75"
      },
      "outputs": [],
      "source": [
        "def MAE_Gradient (X, t, w, lamda=0): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    unit = np.ones((len(X),1),dtype='int')              # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                       # Horizontally concatenates X with unit \n",
        "    y=np.array(np.dot(X_final,w))                       # Matrix multiplication of X_final and w to get the estimate y\n",
        "    g=np.zeros(len(w))                                  # Initialisation of gradient as an array of zeros\n",
        "    for j in range(0,len(w)):                           # 'For' loop for considering one training set of X at a time\n",
        "        g[j]=(np.sign(y-t)).dot(X_final[:,j])/2         # Gradient with respect Mean Absolute error to is calculated for each set 'j' of X\n",
        "    return g                                            # Gradient is returned\n",
        "    \n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1d418bf5351112ae8716877f8bb6359",
          "grade": true,
          "grade_id": "MAE_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "KJn5pgY0or76"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "595829c0dd17df55a03d346fac507b4a",
          "grade": false,
          "grade_id": "L2_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "G2no6Qlior76"
      },
      "outputs": [],
      "source": [
        "def L2_Gradient (X, t, w, lamda): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    grad=(MSE_Gradient(X,t,w))              # Calling of MSE_Gradient function to find the MSE gradient\n",
        "    # Derivative of L2 regularisation term with respect to w calculation:\n",
        "    d = np.sqrt(np.sum(w[0:-1]**2))         # Denominator term of the derivative is square root of the sum of squares of w\n",
        "    n=np.zeros(len(w))                      # Initialisation of the numerator to zeros\n",
        "    n[0:len(w)-1] = w[0:len(w)-1]           # Numerator of the derivative is w\n",
        "    #\n",
        "    ans= grad + lamda*(n/d)                 # L2 gradient is the MSE gradient + gradient of L2 regularisation term\n",
        "    return ans                              # Total L1 gradient is returned\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f292b42c3389b24e369234a7210b87bb",
          "grade": true,
          "grade_id": "L2_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f1LmblN_or77"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "141736f1feda5aa225e008d749f0015c",
          "grade": false,
          "grade_id": "L1_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "PVLesHrRor79"
      },
      "outputs": [],
      "source": [
        "def L1_Gradient (X, t, w, lamda): # Output should have the same size as w\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    grad=(MSE_Gradient(X,t,w))\n",
        "    # Derivative of L1 regularisation with respect to w calculation:\n",
        "    r =np.sign(w[0:len(w)-1])          # Derivative of w in L1 regularisation term\n",
        "    L1=np.zeros(len(w))                # Initialisation of L1 regularisation gradient term\n",
        "    L1[0:len(w)-1] = r * lamda         # L1 regularisation gradient term = Derivative of L1 regularisation * lamda\n",
        "    #\n",
        "    ans = L1+grad                      # Total gradient is MSE gradient + L1 regularisation gradient\n",
        "    return ans                         # Total L1 gradient is returned\n",
        "\n",
        "    \n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9097b61320e470f429a9f46c7da0b219",
          "grade": true,
          "grade_id": "L1_Gradient_Test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yt-lndJsor7-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 '''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "w=np.array([2,-1,0.5,1])\n",
        "np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1980a2f831b5e1ddc9b510c22ef502c7",
          "grade": false,
          "grade_id": "Gradient_Desc_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_iLbk9cwor7-"
      },
      "source": [
        "## Gradient Descent Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "78cdd5ffa4590c10e19281722ccd537f",
          "grade": false,
          "grade_id": "Gradient_Descent",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "O_77J8Pror7_"
      },
      "outputs": [],
      "source": [
        "def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # We use gradient descent function to optimize weight(w) using the provided training data for given learning rate, lamda. \n",
        "    # Later the updated w is used in validation data and NRMSE is calculated to check how well the obtained w works\n",
        "\n",
        "    # Using gradient descent for training data\n",
        "    loss1 = lossfunc(X,t,w,lamda)                            # For given input of X,t,w (initial guess) and lamda, loss is obtained using the specified loss function\n",
        "    iter,epsilon = 0, 100                                    # Two variables iter and epsilon are initialized to help in specifying stopping criteria for learning\n",
        "    while iter <= max_iter and epsilon >= 1e-10:              # While loop checks if the stopping criteria is met. We stop the iteration if either the maximum no. of iteration specified is exceeded or if difference of two consecutive loss falls below specified threshold.  \n",
        "      grad = gradfunc(X, t, w, lamda)                        # Gradient is evaluated using any of specified gradient functions defined previously\n",
        "      w = w-lr*grad                                          # New best weight(w) is found using previous w, learning rate and gradient (Gradient descent method)\n",
        "      loss2 = loss1                                          # Before computing the loss for updated w, we store the loss of previous w in new variable, as it is required later for comparison\n",
        "      loss1 = lossfunc(X,t,w,lamda)                          # Loss for new w is calculated using specified loss function\n",
        "      epsilon = abs(loss1-loss2)                             # Absolute difference between two consecutive loss values for two consecutive w is estimated. If this is less than specified threshold, iteration stops.\n",
        "      iter+=1                                                # iter is increased by 1 to keep track of no. of iterations performed. If this exceeds max_iter specified, then we exit the loop. \n",
        "    train_loss_final = loss1                                 # final value of loss is stored as train_loss_final which is a demanded output\n",
        "    \n",
        "    # Using gradient descent for validation data\n",
        "    validation_loss_final = lossfunc(X_val,t_val,w,lamda)    # loss is computed for given validation inputs using specified loss function\n",
        "    w_final = w                                              # Last obtained w is stored for returning as output\n",
        "    validation_NRMSE = NRMSE_Metric(X_val, t_val, w_final)   # Validation NRMSE is calculated using pre-defined function. It is also a demanded output  \n",
        "    \n",
        "    # returns all desired output from training and validation set\n",
        "    print(w_final,train_loss_final,validation_loss_final,validation_NRMSE)\n",
        "    return w_final,train_loss_final,validation_loss_final,validation_NRMSE   \n",
        "    \n",
        "    \n",
        "    #raise NotImplementedError()\n",
        "    #return w_final, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "15a67336bed20a51f33820bf615186e8",
          "grade": true,
          "grade_id": "Gradient_Check",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PxYfgj1Bor8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04b2eb5-0e64-4445-ffb5-4920bf2bfda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.40058731 0.33104289 0.93040647] 689.9526658737369 23.637895078897937 9.69411723626606\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "X=np.array([[23,24],[1,2]])\n",
        "t=np.array([4,5])\n",
        "X_val=np.array([[3,4],[5,6]])\n",
        "t_val=np.array([3,4])\n",
        "w=np.array([3,2,1])\n",
        "results = Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient) \n",
        "np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)\n",
        "np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept\n",
        "#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "24f327aa6a14590077f907ee8323724c",
          "grade": false,
          "grade_id": "PseudoInvTitle",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PfClOwZ5or8A"
      },
      "source": [
        "## Pseudo Inverse Method\n",
        "\n",
        "You have to implement a slightly more advanced version, with L2 penalty:\n",
        "\n",
        "w = (X' X + lambda I)^(-1) X' t.\n",
        "\n",
        "See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf\n",
        "\n",
        "Here, the column of 1's in assumed to be included in X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "928c9b105c4bcf3c132c7520a140d509",
          "grade": false,
          "grade_id": "PseudoInv",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xX_MU2Ulor8B"
      },
      "outputs": [],
      "source": [
        "def Pseudo_Inverse (X, t, lamda): # Output should be weight vector\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    unit = np.ones((len(X),1),dtype='int')           # 1D array of ones of integer datatype with length equal to that of X\n",
        "    X_final = np.hstack((X,unit))                    # 1D array of previous step is horizontally concatenated to input X\n",
        "    Y = X_final.T.dot(X_final)                       # Matrix multiplication between X_final tanspose and X_final\n",
        "    I = np.identity(Y.shape[0])                      # Identity square matrix of size same as rows of Y\n",
        "    Z =  np.linalg.pinv(Y+lamda*I)                   # Inverse of (Y+lambda*I)\n",
        "    pseudo_inverse = ((X_final.T).dot(t)).dot(Z)     # Matrix multiplication of X_final, t and Z\n",
        "    return pseudo_inverse                            # Returns pseudo inverse of input matrix\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a5178b8280650ed5d1d55dc6bbb38a29",
          "grade": true,
          "grade_id": "PseudoInvTest",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UW3RqeOior8C"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TEST CASES, DO NOT CHANGE\n",
        "'''\n",
        "''' case 1 - other data'''\n",
        "X=np.array([[3,6,5],[4.5,6.6,6]])\n",
        "t=np.array([4,5.5])\n",
        "np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8748fcc0dc96df6e7bb48a4e315927d3",
          "grade": false,
          "grade_id": "cell-e0fa02d7eecfb851",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zTNNE0qGor8D"
      },
      "source": [
        "#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c9047f75587bfee6b7b0a4b244efc7fa",
          "grade": false,
          "grade_id": "cell-10f0d275f5cd36f6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-SzKbifnor8E"
      },
      "source": [
        "#**Part 2 begins ...**\n",
        "\n",
        "**Instructions to be loosely followed (except number 8):**\n",
        "\n",
        "1. Add more code and text cells between this and the last cell.\n",
        "2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.\n",
        "3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.\n",
        "6. Write your observations and conclusions.\n",
        "7. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.\n",
        "8. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XVJos0GBv1zP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM IDEA FORMULATION\n",
        "\n",
        "Getting a rough idea of the given problem by analysing the data\n"
      ],
      "metadata": {
        "id": "HwoYXBHLYvts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "CLlfOlYbor8F",
        "outputId": "1d2589e0-a1e9-483a-9471-05c8d2afdb6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e4fca03d-6d20-43a5-939b-9770ce631eb6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Present_Tmax</th>\n",
              "      <th>Present_Tmin</th>\n",
              "      <th>LDAPS_RHmin</th>\n",
              "      <th>LDAPS_RHmax</th>\n",
              "      <th>LDAPS_Tmax_lapse</th>\n",
              "      <th>LDAPS_Tmin_lapse</th>\n",
              "      <th>LDAPS_WS</th>\n",
              "      <th>LDAPS_LH</th>\n",
              "      <th>LDAPS_CC1</th>\n",
              "      <th>LDAPS_CC2</th>\n",
              "      <th>LDAPS_CC3</th>\n",
              "      <th>LDAPS_CC4</th>\n",
              "      <th>LDAPS_PPT1</th>\n",
              "      <th>LDAPS_PPT2</th>\n",
              "      <th>LDAPS_PPT3</th>\n",
              "      <th>LDAPS_PPT4</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>DEM</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Solar radiation</th>\n",
              "      <th>Next_Tmax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.7</td>\n",
              "      <td>21.4</td>\n",
              "      <td>58.255688</td>\n",
              "      <td>91.116364</td>\n",
              "      <td>28.074101</td>\n",
              "      <td>23.006936</td>\n",
              "      <td>6.818887</td>\n",
              "      <td>69.451805</td>\n",
              "      <td>0.233947</td>\n",
              "      <td>0.203896</td>\n",
              "      <td>0.161697</td>\n",
              "      <td>0.130928</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.6046</td>\n",
              "      <td>126.991</td>\n",
              "      <td>212.3350</td>\n",
              "      <td>2.7850</td>\n",
              "      <td>5992.895996</td>\n",
              "      <td>29.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31.9</td>\n",
              "      <td>21.6</td>\n",
              "      <td>52.263397</td>\n",
              "      <td>90.604721</td>\n",
              "      <td>29.850689</td>\n",
              "      <td>24.035009</td>\n",
              "      <td>5.691890</td>\n",
              "      <td>51.937448</td>\n",
              "      <td>0.225508</td>\n",
              "      <td>0.251771</td>\n",
              "      <td>0.159444</td>\n",
              "      <td>0.127727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.6046</td>\n",
              "      <td>127.032</td>\n",
              "      <td>44.7624</td>\n",
              "      <td>0.5141</td>\n",
              "      <td>5869.312500</td>\n",
              "      <td>30.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31.6</td>\n",
              "      <td>23.3</td>\n",
              "      <td>48.690479</td>\n",
              "      <td>83.973587</td>\n",
              "      <td>30.091292</td>\n",
              "      <td>24.565633</td>\n",
              "      <td>6.138224</td>\n",
              "      <td>20.573050</td>\n",
              "      <td>0.209344</td>\n",
              "      <td>0.257469</td>\n",
              "      <td>0.204091</td>\n",
              "      <td>0.142125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.5776</td>\n",
              "      <td>127.058</td>\n",
              "      <td>33.3068</td>\n",
              "      <td>0.2661</td>\n",
              "      <td>5863.555664</td>\n",
              "      <td>31.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32.0</td>\n",
              "      <td>23.4</td>\n",
              "      <td>58.239788</td>\n",
              "      <td>96.483688</td>\n",
              "      <td>29.704629</td>\n",
              "      <td>23.326177</td>\n",
              "      <td>5.650050</td>\n",
              "      <td>65.727144</td>\n",
              "      <td>0.216372</td>\n",
              "      <td>0.226002</td>\n",
              "      <td>0.161157</td>\n",
              "      <td>0.134249</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.6450</td>\n",
              "      <td>127.022</td>\n",
              "      <td>45.7160</td>\n",
              "      <td>2.5348</td>\n",
              "      <td>5856.964844</td>\n",
              "      <td>31.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31.4</td>\n",
              "      <td>21.9</td>\n",
              "      <td>56.174095</td>\n",
              "      <td>90.155128</td>\n",
              "      <td>29.113934</td>\n",
              "      <td>23.486480</td>\n",
              "      <td>5.735004</td>\n",
              "      <td>107.965535</td>\n",
              "      <td>0.151407</td>\n",
              "      <td>0.249995</td>\n",
              "      <td>0.178892</td>\n",
              "      <td>0.170021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.5507</td>\n",
              "      <td>127.135</td>\n",
              "      <td>35.0380</td>\n",
              "      <td>0.5055</td>\n",
              "      <td>5859.552246</td>\n",
              "      <td>31.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4fca03d-6d20-43a5-939b-9770ce631eb6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e4fca03d-6d20-43a5-939b-9770ce631eb6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e4fca03d-6d20-43a5-939b-9770ce631eb6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Present_Tmax  Present_Tmin  LDAPS_RHmin  ...   Slope  Solar radiation  Next_Tmax\n",
              "0          28.7          21.4    58.255688  ...  2.7850      5992.895996       29.1\n",
              "1          31.9          21.6    52.263397  ...  0.5141      5869.312500       30.5\n",
              "2          31.6          23.3    48.690479  ...  0.2661      5863.555664       31.1\n",
              "3          32.0          23.4    58.239788  ...  2.5348      5856.964844       31.7\n",
              "4          31.4          21.9    56.174095  ...  0.5055      5859.552246       31.2\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Reading the provided training file and seeing what all columns are there\n",
        "\n",
        "learning_data = pd.read_csv('https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv')\n",
        "learning_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing the size of the data provided to get a better feel of the problem\n",
        "learning_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ateHMTxWxedR",
        "outputId": "26f1ce18-0abe-48d4-b8dd-95304a0ad3ca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6082, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for presence of null values so that they can be eliminated\n",
        "# Null values have no use in prediction and will lead to increased computation time\n",
        "learning_data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WPU4ZKB7-MS",
        "outputId": "41e1a4ec-a05b-4aa4-e5c7-88e8aa1bbe7b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Present_Tmax        0\n",
              "Present_Tmin        0\n",
              "LDAPS_RHmin         0\n",
              "LDAPS_RHmax         0\n",
              "LDAPS_Tmax_lapse    0\n",
              "LDAPS_Tmin_lapse    0\n",
              "LDAPS_WS            0\n",
              "LDAPS_LH            0\n",
              "LDAPS_CC1           0\n",
              "LDAPS_CC2           0\n",
              "LDAPS_CC3           0\n",
              "LDAPS_CC4           0\n",
              "LDAPS_PPT1          0\n",
              "LDAPS_PPT2          0\n",
              "LDAPS_PPT3          0\n",
              "LDAPS_PPT4          0\n",
              "lat                 0\n",
              "lon                 0\n",
              "DEM                 0\n",
              "Slope               0\n",
              "Solar radiation     0\n",
              "Next_Tmax           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION**   :  \n",
        "We can see that all values are required to be included in our program"
      ],
      "metadata": {
        "id": "tadQL5cNX5uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "gDDY5rvn7-Bi",
        "outputId": "14dcac29-1f99-4fe6-dbf5-4ed3aec85362"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-463342b0-ceb6-4d05-8e20-e64f58bb7b9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Present_Tmax</th>\n",
              "      <th>Present_Tmin</th>\n",
              "      <th>LDAPS_RHmin</th>\n",
              "      <th>LDAPS_RHmax</th>\n",
              "      <th>LDAPS_Tmax_lapse</th>\n",
              "      <th>LDAPS_Tmin_lapse</th>\n",
              "      <th>LDAPS_WS</th>\n",
              "      <th>LDAPS_LH</th>\n",
              "      <th>LDAPS_CC1</th>\n",
              "      <th>LDAPS_CC2</th>\n",
              "      <th>LDAPS_CC3</th>\n",
              "      <th>LDAPS_CC4</th>\n",
              "      <th>LDAPS_PPT1</th>\n",
              "      <th>LDAPS_PPT2</th>\n",
              "      <th>LDAPS_PPT3</th>\n",
              "      <th>LDAPS_PPT4</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>DEM</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Solar radiation</th>\n",
              "      <th>Next_Tmax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>29.801611</td>\n",
              "      <td>23.115291</td>\n",
              "      <td>56.018614</td>\n",
              "      <td>88.515941</td>\n",
              "      <td>29.614770</td>\n",
              "      <td>23.443802</td>\n",
              "      <td>7.139833</td>\n",
              "      <td>62.465128</td>\n",
              "      <td>0.371016</td>\n",
              "      <td>0.353386</td>\n",
              "      <td>0.303437</td>\n",
              "      <td>0.287287</td>\n",
              "      <td>0.579411</td>\n",
              "      <td>0.486801</td>\n",
              "      <td>0.252828</td>\n",
              "      <td>0.257101</td>\n",
              "      <td>37.544753</td>\n",
              "      <td>126.991630</td>\n",
              "      <td>61.872200</td>\n",
              "      <td>1.260263</td>\n",
              "      <td>5343.173788</td>\n",
              "      <td>30.303058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.951021</td>\n",
              "      <td>2.401665</td>\n",
              "      <td>14.455890</td>\n",
              "      <td>7.312849</td>\n",
              "      <td>2.901865</td>\n",
              "      <td>2.310063</td>\n",
              "      <td>2.252380</td>\n",
              "      <td>33.111077</td>\n",
              "      <td>0.264576</td>\n",
              "      <td>0.259166</td>\n",
              "      <td>0.244996</td>\n",
              "      <td>0.251257</td>\n",
              "      <td>1.877036</td>\n",
              "      <td>1.751910</td>\n",
              "      <td>1.049527</td>\n",
              "      <td>1.154900</td>\n",
              "      <td>0.050415</td>\n",
              "      <td>0.079312</td>\n",
              "      <td>54.231137</td>\n",
              "      <td>1.372206</td>\n",
              "      <td>431.221299</td>\n",
              "      <td>3.097031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.100000</td>\n",
              "      <td>11.300000</td>\n",
              "      <td>19.794666</td>\n",
              "      <td>58.936283</td>\n",
              "      <td>17.624954</td>\n",
              "      <td>14.720029</td>\n",
              "      <td>2.882580</td>\n",
              "      <td>-13.603212</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.456200</td>\n",
              "      <td>126.826000</td>\n",
              "      <td>12.370000</td>\n",
              "      <td>0.098500</td>\n",
              "      <td>4329.520508</td>\n",
              "      <td>17.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>27.900000</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>45.446830</td>\n",
              "      <td>84.404219</td>\n",
              "      <td>27.694146</td>\n",
              "      <td>21.925374</td>\n",
              "      <td>5.649959</td>\n",
              "      <td>37.250612</td>\n",
              "      <td>0.147001</td>\n",
              "      <td>0.137485</td>\n",
              "      <td>0.094394</td>\n",
              "      <td>0.072224</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.510200</td>\n",
              "      <td>126.937000</td>\n",
              "      <td>28.700000</td>\n",
              "      <td>0.271300</td>\n",
              "      <td>4997.796875</td>\n",
              "      <td>28.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>29.900000</td>\n",
              "      <td>23.200000</td>\n",
              "      <td>54.158222</td>\n",
              "      <td>90.049240</td>\n",
              "      <td>29.693295</td>\n",
              "      <td>23.617270</td>\n",
              "      <td>6.571684</td>\n",
              "      <td>56.753248</td>\n",
              "      <td>0.317329</td>\n",
              "      <td>0.305546</td>\n",
              "      <td>0.246939</td>\n",
              "      <td>0.215075</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.550700</td>\n",
              "      <td>126.995000</td>\n",
              "      <td>45.716000</td>\n",
              "      <td>0.618000</td>\n",
              "      <td>5445.730469</td>\n",
              "      <td>30.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>32.000000</td>\n",
              "      <td>24.800000</td>\n",
              "      <td>65.983959</td>\n",
              "      <td>94.024183</td>\n",
              "      <td>31.663208</td>\n",
              "      <td>25.104358</td>\n",
              "      <td>8.131577</td>\n",
              "      <td>84.760510</td>\n",
              "      <td>0.580648</td>\n",
              "      <td>0.555505</td>\n",
              "      <td>0.474692</td>\n",
              "      <td>0.486157</td>\n",
              "      <td>0.079479</td>\n",
              "      <td>0.028317</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>37.577600</td>\n",
              "      <td>127.042000</td>\n",
              "      <td>59.832400</td>\n",
              "      <td>1.767800</td>\n",
              "      <td>5729.980835</td>\n",
              "      <td>32.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>37.600000</td>\n",
              "      <td>29.900000</td>\n",
              "      <td>98.524734</td>\n",
              "      <td>99.996887</td>\n",
              "      <td>37.050301</td>\n",
              "      <td>29.084492</td>\n",
              "      <td>21.857621</td>\n",
              "      <td>213.414006</td>\n",
              "      <td>0.967277</td>\n",
              "      <td>0.966997</td>\n",
              "      <td>0.983789</td>\n",
              "      <td>0.974710</td>\n",
              "      <td>23.701544</td>\n",
              "      <td>21.621661</td>\n",
              "      <td>15.841235</td>\n",
              "      <td>13.998953</td>\n",
              "      <td>37.645000</td>\n",
              "      <td>127.135000</td>\n",
              "      <td>212.335000</td>\n",
              "      <td>5.178200</td>\n",
              "      <td>5992.895996</td>\n",
              "      <td>38.900000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-463342b0-ceb6-4d05-8e20-e64f58bb7b9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-463342b0-ceb6-4d05-8e20-e64f58bb7b9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-463342b0-ceb6-4d05-8e20-e64f58bb7b9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Present_Tmax  Present_Tmin  ...  Solar radiation    Next_Tmax\n",
              "count   6082.000000   6082.000000  ...      6082.000000  6082.000000\n",
              "mean      29.801611     23.115291  ...      5343.173788    30.303058\n",
              "std        2.951021      2.401665  ...       431.221299     3.097031\n",
              "min       20.100000     11.300000  ...      4329.520508    17.400000\n",
              "25%       27.900000     21.500000  ...      4997.796875    28.400000\n",
              "50%       29.900000     23.200000  ...      5445.730469    30.500000\n",
              "75%       32.000000     24.800000  ...      5729.980835    32.600000\n",
              "max       37.600000     29.900000  ...      5992.895996    38.900000\n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OSERVATION**   :\n",
        "Looking into the above data description we can see how each parameter varies and how their trends are"
      ],
      "metadata": {
        "id": "QO3kbNa1ZXzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_data= learning_data.sample(frac = 1)\n",
        "learning_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "d-pxGOR1Yz1T",
        "outputId": "5fff620b-7d19-4e30-c479-e5011710f34f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0c89b093-73e2-42d3-96bc-8085aeb17e9d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Present_Tmax</th>\n",
              "      <th>Present_Tmin</th>\n",
              "      <th>LDAPS_RHmin</th>\n",
              "      <th>LDAPS_RHmax</th>\n",
              "      <th>LDAPS_Tmax_lapse</th>\n",
              "      <th>LDAPS_Tmin_lapse</th>\n",
              "      <th>LDAPS_WS</th>\n",
              "      <th>LDAPS_LH</th>\n",
              "      <th>LDAPS_CC1</th>\n",
              "      <th>LDAPS_CC2</th>\n",
              "      <th>LDAPS_CC3</th>\n",
              "      <th>LDAPS_CC4</th>\n",
              "      <th>LDAPS_PPT1</th>\n",
              "      <th>LDAPS_PPT2</th>\n",
              "      <th>LDAPS_PPT3</th>\n",
              "      <th>LDAPS_PPT4</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>DEM</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Solar radiation</th>\n",
              "      <th>Next_Tmax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3967</th>\n",
              "      <td>32.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>55.828007</td>\n",
              "      <td>95.862183</td>\n",
              "      <td>31.539253</td>\n",
              "      <td>23.819131</td>\n",
              "      <td>6.599190</td>\n",
              "      <td>119.692110</td>\n",
              "      <td>0.087770</td>\n",
              "      <td>0.115293</td>\n",
              "      <td>0.008808</td>\n",
              "      <td>0.051158</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.6181</td>\n",
              "      <td>127.004</td>\n",
              "      <td>146.5540</td>\n",
              "      <td>4.7296</td>\n",
              "      <td>5380.158691</td>\n",
              "      <td>34.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5610</th>\n",
              "      <td>32.9</td>\n",
              "      <td>25.0</td>\n",
              "      <td>46.615067</td>\n",
              "      <td>90.375885</td>\n",
              "      <td>33.169555</td>\n",
              "      <td>26.004523</td>\n",
              "      <td>7.157313</td>\n",
              "      <td>91.154647</td>\n",
              "      <td>0.156578</td>\n",
              "      <td>0.051419</td>\n",
              "      <td>0.001840</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.6046</td>\n",
              "      <td>126.991</td>\n",
              "      <td>212.3350</td>\n",
              "      <td>2.7850</td>\n",
              "      <td>5245.199219</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5887</th>\n",
              "      <td>32.2</td>\n",
              "      <td>26.4</td>\n",
              "      <td>64.911324</td>\n",
              "      <td>91.737434</td>\n",
              "      <td>28.944484</td>\n",
              "      <td>24.504042</td>\n",
              "      <td>5.862176</td>\n",
              "      <td>103.454758</td>\n",
              "      <td>0.345880</td>\n",
              "      <td>0.407272</td>\n",
              "      <td>0.218534</td>\n",
              "      <td>0.050978</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.6450</td>\n",
              "      <td>127.022</td>\n",
              "      <td>45.7160</td>\n",
              "      <td>2.5348</td>\n",
              "      <td>4679.124512</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5489</th>\n",
              "      <td>33.2</td>\n",
              "      <td>24.7</td>\n",
              "      <td>49.911877</td>\n",
              "      <td>89.242279</td>\n",
              "      <td>33.386603</td>\n",
              "      <td>27.867494</td>\n",
              "      <td>5.746215</td>\n",
              "      <td>74.567476</td>\n",
              "      <td>0.367225</td>\n",
              "      <td>0.150426</td>\n",
              "      <td>0.252066</td>\n",
              "      <td>0.329300</td>\n",
              "      <td>0.08236</td>\n",
              "      <td>0.00014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>37.6046</td>\n",
              "      <td>127.032</td>\n",
              "      <td>44.7624</td>\n",
              "      <td>0.5141</td>\n",
              "      <td>5247.996582</td>\n",
              "      <td>34.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5774</th>\n",
              "      <td>32.5</td>\n",
              "      <td>23.5</td>\n",
              "      <td>54.377689</td>\n",
              "      <td>83.143707</td>\n",
              "      <td>32.920552</td>\n",
              "      <td>25.369539</td>\n",
              "      <td>5.201242</td>\n",
              "      <td>92.720334</td>\n",
              "      <td>0.047834</td>\n",
              "      <td>0.121767</td>\n",
              "      <td>0.183315</td>\n",
              "      <td>0.299410</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.701467</td>\n",
              "      <td>0.092443</td>\n",
              "      <td>37.4697</td>\n",
              "      <td>126.995</td>\n",
              "      <td>82.2912</td>\n",
              "      <td>2.2579</td>\n",
              "      <td>4861.894043</td>\n",
              "      <td>32.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c89b093-73e2-42d3-96bc-8085aeb17e9d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c89b093-73e2-42d3-96bc-8085aeb17e9d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c89b093-73e2-42d3-96bc-8085aeb17e9d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Present_Tmax  Present_Tmin  ...  Solar radiation  Next_Tmax\n",
              "3967          32.2          26.2  ...      5380.158691       34.9\n",
              "5610          32.9          25.0  ...      5245.199219       33.0\n",
              "5887          32.2          26.4  ...      4679.124512       34.0\n",
              "5489          33.2          24.7  ...      5247.996582       34.3\n",
              "5774          32.5          23.5  ...      4861.894043       32.5\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OSERVATION**   :\n",
        "Randomizing the given data so that while splitting between train and validation, we do not get similar kinds of data in the same group.\n",
        "\n",
        "This step is very necessary if we have data in some sorted format to improve the machine's learning capacity"
      ],
      "metadata": {
        "id": "KSYrnpnWZ4sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SPLITING\n",
        "\n",
        "From the given set of data we split them into training and validation data.\n",
        "\n",
        "Also we have to separate the target variable given and the design variables. \n",
        "\n",
        "This splitting of data, and normalizing the segregated data will be done in this section.\n",
        "\n"
      ],
      "metadata": {
        "id": "FSJpXf05aOeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size=round(0.8*len(learning_data))\n",
        "val_size = round(0.2*len(learning_data))\n",
        "train_data = learning_data[:train_size]\n",
        "val_data = learning_data[train_size:]\n"
      ],
      "metadata": {
        "id": "PL41gm6G5YNa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "80 % of data is used for training\n",
        "20 % of data is used for validation "
      ],
      "metadata": {
        "id": "M0aiVlXIbfUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.mean(train_data,axis=0)   # Mean of each axis of the input X. axis=0 in means first axis. \n",
        "s = np.std(train_data,axis=0)    # Standard deviation of each axis of X.\n",
        "n_train = (train_data-m)/s \n",
        "n_val = (val_data-m)/s "
      ],
      "metadata": {
        "id": "RLyJdq0ZAdfY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and validation data are normalized with respect to the mean and standard deviation of training data"
      ],
      "metadata": {
        "id": "zMOy4H80by9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the column of target from the variables\n",
        "X_train, t_train = n_train.drop(columns='Next_Tmax').copy(), train_data['Next_Tmax'].copy()\n",
        "X_valid, t_valid = n_val.drop(columns='Next_Tmax').copy(), val_data['Next_Tmax'].copy()\n"
      ],
      "metadata": {
        "id": "XR0JBNFiRrVj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUPPORTING FUNCTIONS\n",
        "\n",
        "GRADIENT DESCENT"
      ],
      "metadata": {
        "id": "ZIT_J8NAckP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This fuction uses gradient descent and gives all the required outputs to find the best hyperparameters\n",
        "\n",
        "def Grad_Descent(X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    loss2 = lossfunc(X,t,w,lamda)\n",
        "    iter,e = 0, 1\n",
        "    while iter <= max_iter and e >= epsilon:\n",
        "      grad = gradfunc (X, t, w, lamda)\n",
        "      w = w-lr*grad\n",
        "      loss1=loss2\n",
        "      loss2= lossfunc(X,t,w,lamda)\n",
        "      e = abs(loss1-loss2)\n",
        "      iter+=1\n",
        "    train_loss_final = loss2\n",
        "    train_RMSE = np.sqrt(train_loss_final)\n",
        "    \n",
        "    #\n",
        "    #Validation\n",
        "    loss2_val = lossfunc(X_val,t_val,w,lamda)\n",
        "    validation_loss_final = loss2_val\n",
        "    w_final=w\n",
        "    #\n",
        "    validation_RMSE=np.sqrt(validation_loss_final)\n",
        "    validation_NRMSE= NRMSE_Metric (X_val, t_val, w_final,lamda)\n",
        "    train_NRMSE= NRMSE_Metric (X, t, w_final,lamda)\n",
        "    \n",
        "    #raise NotImplementedError()\n",
        "    return w_final, train_RMSE, validation_RMSE, train_NRMSE, validation_NRMSE #You should return variables structured like this."
      ],
      "metadata": {
        "id": "VA8zzxLI3uIK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USING L1 LOSS"
      ],
      "metadata": {
        "id": "f7ufvkD1d_V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = np.arange(0.01,1,0.05) \n",
        "train_RMSE_L1=np.zeros((len(l)))\n",
        "val_RMSE_L1=np.zeros((len(l)))\n",
        "val_NRMSE_L1=np.zeros((len(l)))\n",
        "train_NRMSE_L1=np.zeros((len(l)))\n",
        "w_f_L1=np.zeros((len(l),X_train.shape[1]+1))\n",
        "lr =  1e-5\n",
        "max_iter = 100\n",
        "epsilon = 1e-5\n",
        "i=0\n",
        "\n",
        "for lamda in l: \n",
        "  w = 1*np.random.rand(X_train.shape[1]+1)-5 \n",
        "  w=(w-np.mean(w))/(np.std(w))\n",
        "  w_f_L1[i,:], train_RMSE_L1[i], val_RMSE_L1[i] ,train_NRMSE_L1[i], val_NRMSE_L1[i] = Grad_Descent (X_train, X_valid, t_train, t_valid, w, lamda, max_iter, epsilon, lr, L1_Loss ,L1_Gradient)\n",
        "  i+=1\n",
        "\n",
        "plt.plot(1/l,train_RMSE_L1,'g-',label=\"Train_RMSE\")\n",
        "plt.plot(1/l,val_RMSE_L1,'r-',label=\"Validation_RMSE\")\n",
        "plt.xlabel('1/lamda')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "print(train_RMSE_L1, val_RMSE_L1, val_NRMSE_L1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "WwfuAzNsOxUf",
        "outputId": "ed3dce07-6775-4c1d-959f-a4593a5830d4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.53817612 1.64538584 1.82549557 1.82715883 1.88941862 1.96646802\n",
            " 1.98048806 2.14910102 2.20737028 2.23410024 2.42252148 2.5417513\n",
            " 2.46171388 2.47726299 2.63037646 2.59682392 2.58805738 2.71706898\n",
            " 2.69707144 2.82854866] [1.5429958  1.69868715 1.84354062 1.84642662 1.92236603 2.00873494\n",
            " 2.01208053 2.15950701 2.23557968 2.26426233 2.43943521 2.56232762\n",
            " 2.48972429 2.50002188 2.67265337 2.63910207 2.61565487 2.7444072\n",
            " 2.70868062 2.85119765] [0.50028094 0.51911919 0.51920374 0.5050699  0.49784651 0.51126557\n",
            " 0.50949067 0.51195953 0.51472982 0.51549201 0.49697715 0.51334559\n",
            " 0.53052612 0.50825736 0.544308   0.51697706 0.48584683 0.49589894\n",
            " 0.48899223 0.49723251]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.where(min(val_RMSE_L1))\n",
        "w_L1 = w_f_L1[i,:]\n",
        "print(\"Best lamda near to: \",l[i])\n",
        "print(\"Validation: \",min(val_RMSE_L1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQMrjJIneaWx",
        "outputId": "98386206-c37f-4919-cca3-9ac0ad52b564"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best lamda near to:  [0.01]\n",
            "Validation:  1.5429957972644741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USING L2 LOSS"
      ],
      "metadata": {
        "id": "CH2rnft3kD6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = np.arange(0.01,1,0.05) \n",
        "train_RMSE_L2=np.zeros((len(l)))\n",
        "val_RMSE_L2=np.zeros((len(l)))\n",
        "val_NRMSE_L2=np.zeros((len(l)))\n",
        "train_NRMSE_L2=np.zeros((len(l)))\n",
        "w_f_L2=np.zeros((len(l),X_train.shape[1]+1))\n",
        "lr =  1e-5\n",
        "max_iter = 100\n",
        "epsilon = 1e-5\n",
        "i=0\n",
        "\n",
        "for lamda in l: \n",
        "  w = 1*np.random.rand(X_train.shape[1]+1)-5 \n",
        "  w=(w-np.mean(w))/(np.std(w))\n",
        "  w_f_L2[i,:], train_RMSE_L2[i], val_RMSE_L2[i] ,train_NRMSE_L2[i], val_NRMSE_L2[i] = Grad_Descent (X_train, X_valid, t_train, t_valid, w, lamda, max_iter, epsilon, lr, L2_Loss ,L2_Gradient)\n",
        "  i+=1\n",
        "\n",
        "plt.plot(1/l,train_RMSE_L2,'g-',label=\"Train_RMSE\")\n",
        "plt.plot(1/l,val_RMSE_L2,'r-',label=\"Validation_RMSE\")\n",
        "plt.xlabel('1/lamda')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "print(train_RMSE_L2, val_RMSE_L2, val_NRMSE_L2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "b1hQKvTlGQ17",
        "outputId": "7273449a-387d-4f6b-b6d9-96250a7bfd82"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.58385633 1.49812773 1.65424089 1.60887229 1.70641106 1.65920028\n",
            " 1.69054972 1.66984393 1.87618387 1.82034368 1.83399272 1.88120272\n",
            " 1.8815687  1.8169439  1.90132973 1.98454912 2.1575409  2.04996739\n",
            " 2.13780493 1.99981655] [1.62973624 1.52964161 1.68901601 1.68617942 1.75689493 1.69121756\n",
            " 1.7073857  1.70639779 1.92891377 1.89259098 1.8560584  1.92677675\n",
            " 1.90278727 1.86396747 1.93157986 2.02778067 2.20504507 2.08491416\n",
            " 2.18993336 2.02197405] [0.53421806 0.48975006 0.53381484 0.5261153  0.53434275 0.49542644\n",
            " 0.48747892 0.49576973 0.55477108 0.54360812 0.51184071 0.52579486\n",
            " 0.51573654 0.50741556 0.51786462 0.53401898 0.51715378 0.53075234\n",
            " 0.55182869 0.51137121]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.where(min(val_RMSE_L2))\n",
        "w_L2 = w_f_L2[i,:]\n",
        "w_L2 = w_L2[0,0,:]\n",
        "print(\"Best lamda near to: \",l[i])\n",
        "print(\"Validation: \",min(val_RMSE_L2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "___Nc_1CkcF0",
        "outputId": "1cf8d3fa-a780-4fe8-c116-18d42ca0e117"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best lamda near to:  [0.01]\n",
            "Validation:  1.5296416119653906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can hence finalize the best lamda to be 0.1\n",
        "\n",
        "Both L1 loss and L2 loss gives the same result"
      ],
      "metadata": {
        "id": "bUL8bYB1kmoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PSEUDO INVERSE"
      ],
      "metadata": {
        "id": "s5Cn1lxd8Fwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing MRSE for Lamda ranges from 1e-5 to 1e+5 (wide Range)\n",
        "\n",
        "l_p = np.arange(0.01,1,0.05) \n",
        "train_RMSE_p = []\n",
        "val_RMSE_p= []\n",
        "val_NRMSE_p = []\n",
        "val_NRMSE_p = []\n",
        "\n",
        "for lamda in l_p:\n",
        "    w_p = Pseudo_Inverse(X_train,t_train,lamda)\n",
        "    train_RMSE_p.append(np.sqrt(MSE_Loss (X_train, t_train, w_p, lamda =0)))\n",
        "    val_RMSE_p.append(np.sqrt(MSE_Loss (X_valid, t_valid, w_p , lamda =0)))\n",
        "    val_NRMSE_p.append(NRMSE_Metric(X_valid, t_valid, w_p, lamda=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "8x7MfoNL3IfT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.where(min(val_RMSE_p))\n",
        "print(\"Best lamda near to: \",l[i])\n",
        "print(\"Validation: \",min(val_RMSE_p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZxAb_gk9Yig",
        "outputId": "c9700192-3732-44ce-aa55-3a1f7d3f16a6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best lamda near to:  [0.01]\n",
            "Validation:  1.4374138590774665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction using test data\n"
      ],
      "metadata": {
        "id": "7QsIWCImloSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting\n",
        "\n",
        "X_test = pd.read_csv('https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv')\n",
        "#X_test = testing_data.to_numpy()\n",
        "#X_test.head()\n",
        "m_t = m[:len(m)-1]\n",
        "s_t = s[:len(s)-1]\n",
        "X_test_normalize = (X_test-m_t)/s_t\n",
        "unit = np.ones((len(X_test_normalize),1),dtype='int')           # 1D array of ones of integer datatype with length equal to that of X\n",
        "X_test_normalize = np.hstack((X_test_normalize,unit))\n",
        "\n",
        "y=np.dot(X_test_normalize,w_L2)\n",
        "raw_data = {'Next_Tmax': y}\n",
        "df = pd.DataFrame(raw_data, columns = ['Next_Tmax'])\n",
        "\n",
        "df.to_csv('213010036_213010037_1.csv',index=False)"
      ],
      "metadata": {
        "id": "D2ikNNs9lVBr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFERRED FRIENDS**\n",
        "\n",
        "193010023_203010030 \n",
        "\n",
        "**REFERRED ONLINE SOURCES**\n",
        "\n",
        "Wikepedia for basic \n",
        "\n",
        "https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer\n",
        "\n",
        "https://machinelearningmastery.com/linear-regression-for-machine-learning/"
      ],
      "metadata": {
        "id": "TPk5fox2rDGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#**... Part 2 ends.**\n",
        "\n",
        "1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code): \n",
        "2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code): "
      ],
      "metadata": {
        "id": "Awb9jUXWq4Aj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "213010036_213010037_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S78YoCEOor7H",
        "2ngus4Hror7p",
        "iJM3mXyDor73",
        "_iLbk9cwor7-",
        "PfClOwZ5or8A",
        "zTNNE0qGor8D",
        "-SzKbifnor8E"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}